{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from ast import literal_eval\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm\n"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-08-16T08:22:44.744815Z",
     "iopub.execute_input": "2023-08-16T08:22:44.745861Z",
     "iopub.status.idle": "2023-08-16T08:22:48.803551Z",
     "shell.execute_reply.started": "2023-08-16T08:22:44.745811Z",
     "shell.execute_reply": "2023-08-16T08:22:48.802543Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "wandb.login(key='7eb3203fd8e9f410dc5b609cccd4054725ccdc96')\n",
    "run = wandb.init(project=\"Swapno_Rec\", job_type=\"train\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-16T08:22:48.80544Z",
     "iopub.execute_input": "2023-08-16T08:22:48.806489Z",
     "iopub.status.idle": "2023-08-16T08:23:23.537357Z",
     "shell.execute_reply.started": "2023-08-16T08:22:48.806453Z",
     "shell.execute_reply": "2023-08-16T08:23:23.536488Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: W&B API key is configured. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /home/shadman/.netrc\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33msknahin\u001B[0m (\u001B[33mcloth_transfer\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669100866662727, max=1.0â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f58329b533b34302bd598e8942f8e9d0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.8"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/shadman/Projects/item-tokenization-model/wandb/run-20230823_165255-ow94clvj</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/cloth_transfer/Swapno_Rec/runs/ow94clvj' target=\"_blank\">warm-lake-1</a></strong> to <a href='https://wandb.ai/cloth_transfer/Swapno_Rec' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/cloth_transfer/Swapno_Rec' target=\"_blank\">https://wandb.ai/cloth_transfer/Swapno_Rec</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/cloth_transfer/Swapno_Rec/runs/ow94clvj' target=\"_blank\">https://wandb.ai/cloth_transfer/Swapno_Rec/runs/ow94clvj</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "if torch.cuda.is_available():\n",
    "    \n",
    "    device=torch.device(\"cuda:0\")\n",
    "    print(\"Running on the GPU\")\n",
    "    torch.cuda.empty_cache()\n",
    "  #dataType=torch.float32\n",
    "else:\n",
    "    device=torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "  #dataType=torch.float32"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-16T08:23:23.538883Z",
     "iopub.execute_input": "2023-08-16T08:23:23.539874Z",
     "iopub.status.idle": "2023-08-16T08:23:23.577127Z",
     "shell.execute_reply.started": "2023-08-16T08:23:23.53984Z",
     "shell.execute_reply": "2023-08-16T08:23:23.576202Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the CPU\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "def save_json(data,name):\n",
    "    with open(name+'.json', 'w',encoding='utf8') as f:\n",
    "        json.dump(data, f,ensure_ascii=False)\n",
    "        \n",
    "def save_jsonl(data,name):\n",
    "    with open(name+'.jsonl', 'w') as f:\n",
    "        for entry in data:\n",
    "            json.dump(entry, f)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "def load_json(path):\n",
    "    f = open (path, \"r\")\n",
    "    data = json.loads(f.read())\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "def load_jsonl(path):\n",
    "    f = open(path,\"r\").readlines()\n",
    "    data=[]\n",
    "    for i in f:\n",
    "        d = json.loads(i)\n",
    "        data.append(d)\n",
    "    return data\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-16T08:23:23.583671Z",
     "iopub.execute_input": "2023-08-16T08:23:23.585949Z",
     "iopub.status.idle": "2023-08-16T08:23:23.611043Z",
     "shell.execute_reply.started": "2023-08-16T08:23:23.585915Z",
     "shell.execute_reply": "2023-08-16T08:23:23.610065Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "vect_model = Word2Vec.load(\"/kaggle/input/swapno-rec-data/global_embed10_25/global_embed10_25.model\")\n",
    "\n",
    "products = pd.read_csv(\"/kaggle/input/shwapno-small/products_small.csv\")\n",
    "products.columns = products.iloc[0].to_list()\n",
    "products = products.drop([0])\n",
    "products = products.reset_index(drop=True)\n",
    "products[\"ProductCode\"] = products[\"ProductCode\"].apply(lambda x: \"P-\"+str(x))\n",
    "\n",
    "customers = pd.read_csv(\"/kaggle/input/shwapno-small/customer_small.csv\")\n",
    "customers[\"CustomerCode\"] = customers[\"CustomerCode\"].apply(lambda x: \"C-\"+str(x))\n",
    "\n",
    "sorted_csv = pd.read_csv(\"/kaggle/input/swapno-rec-data/new_sorted_basket.csv\")\n",
    "sorted_csv[\"InvoiceDate\"]=sorted_csv[\"InvoiceDate\"].apply(lambda x: x[2:-2])\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-16T08:23:23.615674Z",
     "iopub.execute_input": "2023-08-16T08:23:23.618431Z",
     "iopub.status.idle": "2023-08-16T08:23:57.572741Z",
     "shell.execute_reply.started": "2023-08-16T08:23:23.618397Z",
     "shell.execute_reply": "2023-08-16T08:23:57.571705Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/swapno-rec-data/global_embed10_25/global_embed10_25.model'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m vect_model \u001B[38;5;241m=\u001B[39m \u001B[43mWord2Vec\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/kaggle/input/swapno-rec-data/global_embed10_25/global_embed10_25.model\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m products \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/kaggle/input/shwapno-small/products_small.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m products\u001B[38;5;241m.\u001B[39mcolumns \u001B[38;5;241m=\u001B[39m products\u001B[38;5;241m.\u001B[39miloc[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mto_list()\n",
      "File \u001B[0;32m~/Projects/item-tokenization-model/venv/lib/python3.9/site-packages/gensim/models/word2vec.py:1953\u001B[0m, in \u001B[0;36mWord2Vec.load\u001B[0;34m(cls, rethrow, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1934\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\u001B[39;00m\n\u001B[1;32m   1935\u001B[0m \n\u001B[1;32m   1936\u001B[0m \u001B[38;5;124;03mSee Also\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1950\u001B[0m \n\u001B[1;32m   1951\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1952\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1953\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mWord2Vec\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1954\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model, Word2Vec):\n\u001B[1;32m   1955\u001B[0m         rethrow \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/Projects/item-tokenization-model/venv/lib/python3.9/site-packages/gensim/utils.py:486\u001B[0m, in \u001B[0;36mSaveLoad.load\u001B[0;34m(cls, fname, mmap)\u001B[0m\n\u001B[1;32m    482\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloading \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m object from \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, fname)\n\u001B[1;32m    484\u001B[0m compress, subname \u001B[38;5;241m=\u001B[39m SaveLoad\u001B[38;5;241m.\u001B[39m_adapt_by_suffix(fname)\n\u001B[0;32m--> 486\u001B[0m obj \u001B[38;5;241m=\u001B[39m \u001B[43munpickle\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    487\u001B[0m obj\u001B[38;5;241m.\u001B[39m_load_specials(fname, mmap, compress, subname)\n\u001B[1;32m    488\u001B[0m obj\u001B[38;5;241m.\u001B[39madd_lifecycle_event(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloaded\u001B[39m\u001B[38;5;124m\"\u001B[39m, fname\u001B[38;5;241m=\u001B[39mfname)\n",
      "File \u001B[0;32m~/Projects/item-tokenization-model/venv/lib/python3.9/site-packages/gensim/utils.py:1460\u001B[0m, in \u001B[0;36munpickle\u001B[0;34m(fname)\u001B[0m\n\u001B[1;32m   1446\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munpickle\u001B[39m(fname):\n\u001B[1;32m   1447\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load object from `fname`, using smart_open so that `fname` can be on S3, HDFS, compressed etc.\u001B[39;00m\n\u001B[1;32m   1448\u001B[0m \n\u001B[1;32m   1449\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1458\u001B[0m \n\u001B[1;32m   1459\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1460\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m   1461\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m _pickle\u001B[38;5;241m.\u001B[39mload(f, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlatin1\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/Projects/item-tokenization-model/venv/lib/python3.9/site-packages/smart_open/smart_open_lib.py:177\u001B[0m, in \u001B[0;36mopen\u001B[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001B[0m\n\u001B[1;32m    174\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m transport_params \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    175\u001B[0m     transport_params \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m--> 177\u001B[0m fobj \u001B[38;5;241m=\u001B[39m \u001B[43m_shortcut_open\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    178\u001B[0m \u001B[43m    \u001B[49m\u001B[43muri\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    179\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    181\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbuffering\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbuffering\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    183\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    184\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnewline\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    185\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fobj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    187\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fobj\n",
      "File \u001B[0;32m~/Projects/item-tokenization-model/venv/lib/python3.9/site-packages/smart_open/smart_open_lib.py:363\u001B[0m, in \u001B[0;36m_shortcut_open\u001B[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001B[0m\n\u001B[1;32m    360\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m errors \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[1;32m    361\u001B[0m     open_kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124merrors\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m errors\n\u001B[0;32m--> 363\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_builtin_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlocal_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffering\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbuffering\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mopen_kwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/kaggle/input/swapno-rec-data/global_embed10_25/global_embed10_25.model'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "global_key2idx = vect_model.wv.key_to_index\n",
    "global_key2idx.update({\"<s>\":43620,\"</s>\":43621,\"<v>\":43622,\"<pad>\":43623})\n",
    "global_idx2key=vect_model.wv.index_to_key\n",
    "global_idx2key.extend([\"<s>\",\"</s>\",\"<v>\",\"<pad>\"])\n",
    "save_json(global_idx2key,\"global_idx2key\")\n",
    "save_json(global_key2idx,\"global_key2idx\")\n",
    "\n",
    "all_prods = list(products[\"ProductCode\"])\n",
    "all_per = list(customers[\"CustomerCode\"])\n",
    "all_dates = list(set(list(sorted_csv[\"InvoiceDate\"])))\n",
    "\n",
    "\n",
    "global_code2name = dict(zip(list(products[\"ProductCode\"]),list(products[\"ProductName\"])))\n",
    "customer_code2name = dict(zip(list(customers[\"CustomerCode\"]),list(customers[\"CustomerName\"])))\n",
    "global_code2name.update(customer_code2name)\n",
    "global_code2name.update(dict(zip(all_dates,all_dates)))\n",
    "global_code2name.update({\"<s>\":\"start\",\"</s>\":\"end\",\"<v>\":\"void\",\"<pad>\":\"pad\"})\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-16T08:23:57.574302Z",
     "iopub.execute_input": "2023-08-16T08:23:57.575Z",
     "iopub.status.idle": "2023-08-16T08:23:59.89782Z",
     "shell.execute_reply.started": "2023-08-16T08:23:57.574964Z",
     "shell.execute_reply": "2023-08-16T08:23:59.89668Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sorted_csv_2 = sorted_csv.copy()\n",
    "sorted_csv_2[\"CustomerCode\"]=sorted_csv_2[\"CustomerCode\"].apply(lambda x: literal_eval(x)[0])\n",
    "sorted_csv_2[\"basket_sorted\"]=sorted_csv_2[\"basket_sorted\"].apply(lambda x: literal_eval(x))\n",
    "\n",
    "sorted_csv_3 = sorted_csv_2.sort_values(\"InvoiceDate\").groupby(\"CustomerCode\").agg(list).reset_index()\n",
    "sorted_csv_3 = sorted_csv_3.drop(columns=[\"InvoiceNo\",\"ProductCode\",\"InvoiceDate\",\"basket\"])\n",
    "sorted_csv_3.to_csv(\"train_data.csv\", index = False)\n",
    "sorted_csv_3.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-16T08:23:59.899557Z",
     "iopub.execute_input": "2023-08-16T08:23:59.900216Z",
     "iopub.status.idle": "2023-08-16T08:24:41.453403Z",
     "shell.execute_reply.started": "2023-08-16T08:23:59.900181Z",
     "shell.execute_reply": "2023-08-16T08:24:41.452323Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def sort_by_priority_list(values, priority):\n",
    "    priority_dict = dict(\n",
    "        zip(\n",
    "            priority,\n",
    "            range(len(priority)),\n",
    "        ),\n",
    "    )\n",
    "    priority_getter = priority_dict.get # dict.get(key)\n",
    "    return sorted(values, key=priority_getter)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    prev_prods = [i[\"prev_prods\"] for i in batch]\n",
    "    next_basket = [i[\"next_basket\"] for i in batch]\n",
    "    src_mask = [i[\"src_mask\"] for i in batch]\n",
    "    tgt_mask = [i[\"tgt_mask\"] for i in batch]\n",
    "    \n",
    "    batch = {}\n",
    "    batch[\"prev_prods\"] = torch.tensor(list(zip(*itertools.zip_longest(*prev_prods, fillvalue=43623))))\n",
    "    batch[\"next_basket\"] = torch.tensor(list(zip(*itertools.zip_longest(*next_basket, fillvalue=43623))))\n",
    "    batch[\"src_mask\"] = torch.tensor(list(zip(*itertools.zip_longest(*src_mask, fillvalue=0))),dtype = torch.float32)\n",
    "    batch[\"tgt_mask\"] = torch.tensor(list(zip(*itertools.zip_longest(*tgt_mask, fillvalue=0))),dtype = torch.float32)\n",
    "    \n",
    "    return batch"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-16T08:24:41.457038Z",
     "iopub.execute_input": "2023-08-16T08:24:41.460101Z",
     "iopub.status.idle": "2023-08-16T08:24:41.47088Z",
     "shell.execute_reply.started": "2023-08-16T08:24:41.460074Z",
     "shell.execute_reply": "2023-08-16T08:24:41.469886Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "class Swapno_Data(Dataset):\n",
    "    def __init__(self,train=True):\n",
    "        super().__init__()\n",
    "        self.train=train\n",
    "        self.data_df = pd.read_csv(\"/kaggle/working/train_data.csv\")\n",
    "        self.data_df[\"basket_sorted\"]=self.data_df[\"basket_sorted\"].apply(lambda x: literal_eval(x))\n",
    "        self.key2idx = load_json(\"/kaggle/working/global_key2idx.json\")\n",
    "        self.priority_dict = load_json(\"/kaggle/input/swapno-rec-data/priority_dict.json\")\n",
    "        self.max_prev_basket = 5\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        customer_no = idx\n",
    "        customer_id = self.data_df.iloc[customer_no][\"CustomerCode\"]\n",
    "        \n",
    "        pur_bas = self.data_df.iloc[customer_no][\"basket_sorted\"]\n",
    "\n",
    "        total_pur = len(pur_bas)\n",
    "        take_one_pur = random.randint(0,total_pur-1)\n",
    "        \n",
    "        if take_one_pur==0:\n",
    "            prev_basket = [\"<v>\",\"</s>\"]\n",
    "            curr_basket = pur_bas[take_one_pur]+[\"</s>\"]\n",
    "        else:\n",
    "            prev_basket = []\n",
    "            \n",
    "            for tp in range(max([0,take_one_pur-self.max_prev_basket]),take_one_pur):\n",
    "                prev_basket.extend(pur_bas[tp][2:])\n",
    "\n",
    "            prev_basket = sort_by_priority_list(list(set(prev_basket)), self.priority_dict[customer_id])+[\"</s>\"]\n",
    "            curr_basket = pur_bas[take_one_pur]+[\"</s>\"]\n",
    "            \n",
    "        model_input = {}\n",
    "        model_input[\"prev_prods\"] = [self.key2idx[i] for i in prev_basket] \n",
    "        model_input[\"next_basket\"] = [self.key2idx[i] for i in curr_basket] \n",
    "        model_input[\"src_mask\"] = [1]*len(prev_basket) \n",
    "        model_input[\"tgt_mask\"] = [1]*len(curr_basket)\n",
    "        \n",
    "        return model_input\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "    "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-16T08:24:41.47231Z",
     "iopub.execute_input": "2023-08-16T08:24:41.472632Z",
     "iopub.status.idle": "2023-08-16T08:24:41.492103Z",
     "shell.execute_reply.started": "2023-08-16T08:24:41.472601Z",
     "shell.execute_reply": "2023-08-16T08:24:41.490303Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset = Swapno_Data(train=True)\n",
    "train_loader = DataLoader(train_dataset, collate_fn=collate_batch, batch_size=10, shuffle=True, pin_memory=True, drop_last=True, num_workers=2)\n",
    "\n",
    "check_loader = DataLoader(train_dataset, collate_fn=collate_batch, batch_size=10, shuffle=True, pin_memory=True, drop_last=True, num_workers=2)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-16T08:24:41.499026Z",
     "iopub.execute_input": "2023-08-16T08:24:41.499412Z",
     "iopub.status.idle": "2023-08-16T08:24:59.156292Z",
     "shell.execute_reply.started": "2023-08-16T08:24:41.499363Z",
     "shell.execute_reply": "2023-08-16T08:24:59.154861Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for i in train_loader:\n",
    "    print(i)\n",
    "    break"
   ],
   "metadata": {
    "scrolled": true,
    "execution": {
     "iopub.status.busy": "2023-08-16T08:24:59.157959Z",
     "iopub.execute_input": "2023-08-16T08:24:59.158558Z",
     "iopub.status.idle": "2023-08-16T08:25:02.461706Z",
     "shell.execute_reply.started": "2023-08-16T08:24:59.158522Z",
     "shell.execute_reply": "2023-08-16T08:25:02.45776Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class EncoderDecoderTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers):\n",
    "        super(EncoderDecoderTransformer, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.global_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=2048, batch_first=True),num_encoder_layers)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.TransformerDecoder(nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward=2048, batch_first=True),num_decoder_layers)\n",
    "\n",
    "        self.output_linear = nn.Linear(d_model, vocab_size)\n",
    "        self.soft = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, src,src_key_padding_mask,tgt,tgt_key_padding_mask):\n",
    "        tgt_len = tgt.shape[1]\n",
    "        \n",
    "#         print(tgt_key_padding_mask[:,:tgt_len])\n",
    "        tgt_mask = torch.tril(torch.ones((tgt_len, tgt_len),dtype = torch.float32)).to(device)\n",
    "        encoder_output = self.encoder(src=self.global_embedding(src), src_key_padding_mask = src_key_padding_mask)\n",
    "#         print(encoder_output)\n",
    "        decoder_output = self.decoder(tgt=self.global_embedding(tgt), memory=encoder_output, tgt_mask = tgt_mask,tgt_key_padding_mask=tgt_key_padding_mask[:,:tgt_len])\n",
    "        output = self.output_linear(decoder_output)\n",
    "        return output\n",
    "    \n",
    "    def decode(self,src,src_key_padding_mask,tgt,max_length = 512):\n",
    "        with torch.no_grad():\n",
    "            encoder_output = self.encoder(src=self.global_embedding(src), src_key_padding_mask = src_key_padding_mask)\n",
    "        generation = 0\n",
    "        while(generation<max_length):\n",
    "            tgt_len = tgt.shape[1]\n",
    "            tgt_mask = torch.tril(torch.ones((tgt_len, tgt_len),dtype = torch.float32)).to(device) \n",
    "            with torch.no_grad():\n",
    "#                 print(tgt)\n",
    "                decoder_output = self.decoder(tgt=self.global_embedding(tgt), memory=encoder_output, tgt_mask = tgt_mask,tgt_key_padding_mask=None)\n",
    "                output = self.output_linear(decoder_output)\n",
    "                predicted_tokens = torch.argmax(self.soft(output)[:,-1,:], dim=-1).unsqueeze(1)\n",
    "                tgt = torch.cat([tgt,predicted_tokens],dim=1)\n",
    "            generation=generation+1\n",
    "        return tgt\n",
    "    \n",
    "\n",
    "def get_preds(pred,target):\n",
    "\n",
    "    avg_pred = 0\n",
    "    F1 = 0\n",
    "    for k in range(len(pred)):\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        FN = 0\n",
    "        pred_idx = pred[k][2:]\n",
    "        avg_pred = avg_pred+len(pred_idx)-1\n",
    "        target_i=target[k][2:]\n",
    "\n",
    "        for i in pred_idx:\n",
    "            if i not in [43620,43621,43622,43623]:\n",
    "                if i in target_i:\n",
    "                    TP = TP+1\n",
    "                else:\n",
    "                    FP = FP+1\n",
    "\n",
    "        for i in target_i:\n",
    "            if i not in pred_idx and i not in [43620,43621,43622,43623]:\n",
    "                FN = FN+1\n",
    "    \n",
    "        F1 = F1 + TP/(TP+0.5*(FP+FN)+1e-10)\n",
    "    \n",
    "    F1 = F1/len(pred)\n",
    "    avg_pred = avg_pred/len(pred)\n",
    "\n",
    "    return F1,avg_pred\n",
    "                    "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-16T09:07:58.02854Z",
     "iopub.execute_input": "2023-08-16T09:07:58.028966Z",
     "iopub.status.idle": "2023-08-16T09:07:58.055587Z",
     "shell.execute_reply.started": "2023-08-16T09:07:58.028924Z",
     "shell.execute_reply": "2023-08-16T09:07:58.054008Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "vocab_size = 43624 \n",
    "d_model = 1024\n",
    "nhead = 8\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "\n",
    "model = EncoderDecoderTransformer(vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers)\n",
    "model.load_state_dict(torch.load(\"/kaggle/input/swapno-rec-encoder-decoder/swapno_next_bus.pth\"))\n",
    "# torch.save(model.state_dict(), \"swapno_next_bus.pth\")\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.AdamW(model.parameters(), lr=0.00001, betas=(0.5, 0.999))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimiser, T_max=500, eta_min=0.00005)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-16T09:07:58.169331Z",
     "iopub.execute_input": "2023-08-16T09:07:58.169714Z",
     "iopub.status.idle": "2023-08-16T09:08:03.185958Z",
     "shell.execute_reply.started": "2023-08-16T09:07:58.169682Z",
     "shell.execute_reply": "2023-08-16T09:08:03.184726Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "epochs = 100\n",
    "steps = 0\n",
    "for _ in range(epochs):\n",
    "    losses = AverageMeter()\n",
    "    F1s = AverageMeter()\n",
    "    Lens = AverageMeter()\n",
    "    \n",
    "#     model.train()\n",
    "    iterable = enumerate(train_loader)\n",
    "    progress = None\n",
    "    \n",
    "    progress = tqdm(iterable, desc='Train', total=len(train_loader))\n",
    "    iterable = progress\n",
    "        \n",
    "    for i, data in iterable:\n",
    "        src_sequence = data[\"prev_prods\"].to(device)\n",
    "        tgt_sequence = data[\"next_basket\"].to(device)\n",
    "        src_key_padding_mask = data[\"src_mask\"].to(device)\n",
    "        tgt_key_padding_mask = data[\"tgt_mask\"].to(device)\n",
    "        \n",
    "        optimiser.zero_grad()\n",
    "        output = model(src_sequence,src_key_padding_mask, tgt_sequence[:,:-1],tgt_key_padding_mask)  # Exclude the last timestep from target\n",
    "        loss = criterion(output[:,1:].flatten(0,1), tgt_sequence[:, 2:].flatten())  # Shift target by one timestep\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        output_list = torch.argmax(model.soft(output[:,1:]),dim=-1).cpu().numpy().tolist()\n",
    "        tgt_list = tgt_sequence[:, 2:].cpu().numpy().tolist()\n",
    "        \n",
    "        F1,Len = get_preds(output_list,tgt_list)\n",
    "        F1s.update(F1, len(output_list))\n",
    "        Lens.update(Len, len(output_list))\n",
    "        \n",
    "        if steps%500:\n",
    "            wandb.log({\"loss\": losses.avg,\n",
    "                       \"F1\": F1s.avg,\n",
    "                       \"Len\":Lens.avg,\n",
    "                       \"lr\": optimiser.param_groups[0]['lr'],\n",
    "                       \"epoch\": steps})\n",
    "        steps=steps+1\n",
    "        \n",
    "        losses.update(loss.detach().cpu(), tgt_key_padding_mask.shape[0])\n",
    "\n",
    "        if progress is not None:\n",
    "            progress.set_postfix_str('Loss:{ls:0.8f},F1:{f1:0.8f},Len:{ln:0.8f}'.format(\n",
    "                ls=losses.avg,\n",
    "                f1 = F1s.avg*100,\n",
    "                ln = Lens.avg\n",
    "            ))\n",
    "#         break \n",
    "                      \n",
    "    torch.save(model.state_dict(), \"swapno_next_bus.pth\")\n",
    "#     break\n",
    "    \n",
    "\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-16T09:08:03.188547Z",
     "iopub.execute_input": "2023-08-16T09:08:03.189354Z",
     "iopub.status.idle": "2023-08-16T09:11:45.375875Z",
     "shell.execute_reply.started": "2023-08-16T09:08:03.189313Z",
     "shell.execute_reply": "2023-08-16T09:11:45.372313Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(output_list)\n",
    "print(\" \")\n",
    "print(tgt_list)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-16T08:48:04.469488Z",
     "iopub.execute_input": "2023-08-16T08:48:04.469911Z",
     "iopub.status.idle": "2023-08-16T08:48:04.48075Z",
     "shell.execute_reply.started": "2023-08-16T08:48:04.469872Z",
     "shell.execute_reply": "2023-08-16T08:48:04.479195Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# model.eval()\n",
    "def decode(model,src,src_key_padding_mask,tgt,max_length = 512):\n",
    "    soft = nn.Softmax(dim=-1)\n",
    "    with torch.no_grad():\n",
    "        encoder_output = model.encoder(src=model.global_embedding(src), src_key_padding_mask = src_key_padding_mask)\n",
    "    generation = 0\n",
    "#     print(model.global_embedding(src))\n",
    "    print(encoder_output)\n",
    "    while(generation<max_length):\n",
    "        tgt_len = tgt.shape[1]\n",
    "        tgt_mask = torch.tril(torch.ones((tgt_len, tgt_len),dtype = torch.float32)).to(device) \n",
    "        tgt_key_padding_mask = torch.ones((tgt.shape[0], tgt_len),dtype=torch.float32).to(device)\n",
    "#         print(tgt_mask)\n",
    "        with torch.no_grad():\n",
    "            \n",
    "#             print(model.global_embedding(tgt).shape)\n",
    "            decoder_output = model.decoder(tgt=model.global_embedding(tgt), memory=encoder_output, tgt_mask = tgt_mask,tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "            output = model.output_linear(decoder_output)\n",
    "#             print(output[:,-1,:])\n",
    "#             print(torch.sum(soft(output[:,-1,:]),dim=-1))\n",
    "            predicted_tokens = torch.argmax(soft(output[:,-1,:]), dim=-1).unsqueeze(1)\n",
    "#             print(predicted_tokens)\n",
    "            tgt = torch.cat([tgt,predicted_tokens],dim=1)\n",
    "#             print(tgt)\n",
    "        generation=generation+1\n",
    "#         break\n",
    "    return tgt\n",
    "\n",
    "start_tok = tgt_sequence[:,:2]\n",
    "# print(start_tok)\n",
    "# print(src_sequence)\n",
    "y=decode(model,src_sequence,src_key_padding_mask,start_tok,50).cpu().numpy().tolist()\n",
    "y = [i[:2]+list(set(i[2:])) for i in y]\n",
    "print(y)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-16T08:48:16.081389Z",
     "iopub.execute_input": "2023-08-16T08:48:16.081782Z",
     "iopub.status.idle": "2023-08-16T08:48:16.867231Z",
     "shell.execute_reply.started": "2023-08-16T08:48:16.081749Z",
     "shell.execute_reply": "2023-08-16T08:48:16.866102Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.train()\n",
    "F1s = AverageMeter()\n",
    "Lens = AverageMeter()\n",
    "\n",
    "iterable = enumerate(check_loader)\n",
    "progress = None\n",
    "\n",
    "progress = tqdm(iterable, desc='Test', total=len(check_loader))\n",
    "iterable = progress\n",
    "\n",
    "for i, data in iterable:\n",
    "    src_sequence = data[\"prev_prods\"].to(device)\n",
    "    tgt_sequence = data[\"next_basket\"].to(device)\n",
    "    src_key_padding_mask = data[\"src_mask\"].to(device)\n",
    "    tgt_key_padding_mask = data[\"tgt_mask\"].to(device)\n",
    "    start_tok = tgt_sequence[:,:2]\n",
    "    decoded_outputs = model.decode(src_sequence,src_key_padding_mask,start_tok,50).cpu().numpy().tolist()\n",
    "    decoded_outputs = [i[:2]+list(set(i[2:])) for i in decoded_outputs]\n",
    "    tgt_sequence = tgt_sequence.cpu().numpy().tolist()\n",
    "    tgt_sequence = [i[:2]+list(set(i[2:])) for i in tgt_sequence]\n",
    "    F1,Len = get_preds(decoded_outputs,tgt_sequence)\n",
    "    \n",
    "    F1s.update(F1, len(decoded_outputs))\n",
    "    Lens.update(Len, len(decoded_outputs))\n",
    "\n",
    "    if progress is not None:\n",
    "        progress.set_postfix_str('F1:{f1:0.8f}, Len:{le:6.2f}'.format(\n",
    "            f1=F1s.avg*100,\n",
    "            le=Lens.avg\n",
    "        ))\n",
    "    \n",
    "    \n",
    "    "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-16T08:16:00.961045Z",
     "iopub.execute_input": "2023-08-16T08:16:00.962189Z",
     "iopub.status.idle": "2023-08-16T08:16:08.55409Z",
     "shell.execute_reply.started": "2023-08-16T08:16:00.962128Z",
     "shell.execute_reply": "2023-08-16T08:16:08.550602Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def decode(model,src,src_key_padding_mask,tgt,max_length = 512):\n",
    "    with torch.no_grad():\n",
    "        encoder_output = model.encoder(src=model.global_embedding(src), src_key_padding_mask = src_key_padding_mask)\n",
    "    generation = 0\n",
    "    while(generation<max_length):\n",
    "        tgt_len = tgt.shape[1]\n",
    "        tgt_mask = torch.tril(torch.ones((tgt_len, tgt_len),dtype = torch.float32)).to(device) \n",
    "        with torch.no_grad():\n",
    "            decoder_output = model.decoder(tgt=model.global_embedding(tgt), memory=encoder_output, tgt_mask = tgt_mask,tgt_key_padding_mask=None)\n",
    "            output = model.output_linear(decoder_output)\n",
    "            predicted_tokens = torch.argmax(model.soft(output)[:,-1,:], dim=-1).unsqueeze(1)\n",
    "            tgt = torch.cat([tgt,predicted_tokens],dim=1)\n",
    "        generation=generation+1\n",
    "    return tgt"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-16T05:08:39.350429Z",
     "iopub.execute_input": "2023-08-16T05:08:39.350888Z",
     "iopub.status.idle": "2023-08-16T05:08:39.367243Z",
     "shell.execute_reply.started": "2023-08-16T05:08:39.35085Z",
     "shell.execute_reply": "2023-08-16T05:08:39.366185Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.train()\n",
    "\n",
    "for data in check_loader:\n",
    "    src_sequence = data[\"prev_prods\"].to(device)\n",
    "    tgt_sequence = data[\"next_basket\"].to(device)\n",
    "    src_key_padding_mask = data[\"src_mask\"].to(device)\n",
    "    tgt_key_padding_mask = data[\"tgt_mask\"].to(device)\n",
    "    start_tok = tgt_sequence[:,:2]\n",
    "    decoded_outputs = model.decode(src_sequence,src_key_padding_mask,start_tok,50).cpu().numpy().tolist()\n",
    "#     print(decoded_outputs)\n",
    "    decoded_outputs = [i[:2]+list(set(i[2:])) for i in decoded_outputs]\n",
    "    tgt_sequence = tgt_sequence.cpu().numpy().tolist()\n",
    "    tgt_sequence = [i[:2]+list(set(i[2:])) for i in tgt_sequence]\n",
    "    F1,Len = get_preds(decoded_outputs,tgt_sequence)\n",
    "    \n",
    "    for i in range(len(decoded_outputs)):\n",
    "        print(\"Predicted:\\n\")\n",
    "        for dp in decoded_outputs[i]:\n",
    "            if dp not in [43620,43621,43622,43623]:\n",
    "                print(global_code2name[global_idx2key[dp]])\n",
    "        print(\"\\nActual:\\n\")\n",
    "        \n",
    "        for tp in tgt_sequence[i]:\n",
    "            if tp not in [43620,43621,43622,43623]:\n",
    "                print(global_code2name[global_idx2key[tp]])\n",
    "        print(\"\\n\\n\")   \n",
    "        break\n",
    "    break\n",
    "        \n",
    "    \n",
    "    "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-16T08:13:34.85636Z",
     "iopub.execute_input": "2023-08-16T08:13:34.85701Z",
     "iopub.status.idle": "2023-08-16T08:13:36.374518Z",
     "shell.execute_reply.started": "2023-08-16T08:13:34.856957Z",
     "shell.execute_reply": "2023-08-16T08:13:36.373263Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "torch.tensor(np.array([[7,8],[-8,9],[0,-5]]))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-16T06:43:43.321417Z",
     "iopub.execute_input": "2023-08-16T06:43:43.322424Z",
     "iopub.status.idle": "2023-08-16T06:43:43.336032Z",
     "shell.execute_reply.started": "2023-08-16T06:43:43.322389Z",
     "shell.execute_reply": "2023-08-16T06:43:43.335141Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "soft = nn.Softmax(dim = -1)\n",
    "y=soft(torch.tensor(np.array([[7,8],[-8,9],[0,-5]]).astype(np.float32)))\n",
    "print(y)\n",
    "torch.argmax(y,dim=-1)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-16T06:46:04.665102Z",
     "iopub.execute_input": "2023-08-16T06:46:04.665503Z",
     "iopub.status.idle": "2023-08-16T06:46:04.67895Z",
     "shell.execute_reply.started": "2023-08-16T06:46:04.665472Z",
     "shell.execute_reply": "2023-08-16T06:46:04.677665Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.train()\n",
    "def decode(model,src,src_key_padding_mask,tgt,max_length = 512):\n",
    "    soft = nn.Softmax(dim=-1)\n",
    "    with torch.no_grad():\n",
    "        encoder_output = model.encoder(src=model.global_embedding(src), src_key_padding_mask = src_key_padding_mask)\n",
    "    generation = 0\n",
    "#     print(model.global_embedding(src))\n",
    "#     print(encoder_output)\n",
    "    while(generation<max_length):\n",
    "        tgt_len = tgt.shape[1]\n",
    "        tgt_mask = torch.tril(torch.ones((tgt_len, tgt_len),dtype = torch.float32)).to(device) \n",
    "#         print(tgt_mask)\n",
    "        with torch.no_grad():\n",
    "#             print(model.global_embedding(tgt).shape)\n",
    "            decoder_output = model.decoder(tgt=model.global_embedding(tgt), memory=encoder_output, tgt_mask = tgt_mask,tgt_key_padding_mask=None)\n",
    "            output = model.output_linear(decoder_output)\n",
    "#             print(output[:,-1,:])\n",
    "#             print(torch.sum(soft(output[:,-1,:]),dim=-1))\n",
    "            predicted_tokens = torch.argmax(soft(output[:,-1,:]), dim=-1).unsqueeze(1)\n",
    "#             print(predicted_tokens)\n",
    "            tgt = torch.cat([tgt,predicted_tokens],dim=1)\n",
    "#             print(tgt)\n",
    "        generation=generation+1\n",
    "#         break\n",
    "    return tgt\n",
    "\n",
    "start_tok = tgt_sequence[:,:2]\n",
    "# print(start_tok)\n",
    "# print(src_sequence)\n",
    "y=decode(model,src_sequence,src_key_padding_mask,start_tok,50).cpu().numpy().tolist()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-16T08:18:57.022821Z",
     "iopub.execute_input": "2023-08-16T08:18:57.023229Z",
     "iopub.status.idle": "2023-08-16T08:18:57.868983Z",
     "shell.execute_reply.started": "2023-08-16T08:18:57.023186Z",
     "shell.execute_reply": "2023-08-16T08:18:57.867472Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(y)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-16T08:18:59.302107Z",
     "iopub.execute_input": "2023-08-16T08:18:59.302475Z",
     "iopub.status.idle": "2023-08-16T08:18:59.311216Z",
     "shell.execute_reply.started": "2023-08-16T08:18:59.302446Z",
     "shell.execute_reply": "2023-08-16T08:18:59.309656Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# torch.ones((10,1))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-16T07:58:42.254918Z",
     "iopub.execute_input": "2023-08-16T07:58:42.255625Z",
     "iopub.status.idle": "2023-08-16T07:58:42.266681Z",
     "shell.execute_reply.started": "2023-08-16T07:58:42.255588Z",
     "shell.execute_reply": "2023-08-16T07:58:42.265469Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
