{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import holidays\n",
    "from utils import ExtractCalendar\n",
    "from datetime import date\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "# stop_words.extend(\n",
    "#         ['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get',\n",
    "#          'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot',\n",
    "#          'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come',\n",
    "#          'aci'])\n",
    "\n",
    "texts = open('corpus.txt', 'r')\n",
    "texts = texts.readlines()\n",
    "texts\n",
    "\n",
    "\n",
    "# text = ''.join([words for words in text])\n",
    "\n",
    "def clean_text(\n",
    "        string: str,\n",
    "        punctuations=r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~''',\n",
    "        stop_words=stopwords.words('english')\n",
    ") -> str:\n",
    "    string = re.sub(r'https?://\\S+|www\\.\\S+', '', string)\n",
    "    string = re.sub(r'<.*?>', '', string)\n",
    "\n",
    "    for x in string.lower():\n",
    "        if x in punctuations:\n",
    "            string = string.replace(x, \"\")\n",
    "\n",
    "    string = string.lower()\n",
    "    string = ' '.join([word for word in string.split() if word not in stop_words])\n",
    "\n",
    "    string = re.sub(r'\\s+', ' ', string).strip()\n",
    "    string = string.split()\n",
    "    return string\n",
    "\n",
    "\n",
    "window = 2\n",
    "word_lists = []\n",
    "all_text = []\n",
    "\n",
    "for text in texts:\n",
    "    text = clean_text(text)\n",
    "    all_text += text\n",
    "    for i, word in enumerate(text):\n",
    "        for w in range(window):\n",
    "            if i + 1 + w < len(text):\n",
    "                word_lists.append([word] + [text[(i + 1 + w)]])\n",
    "            if i - w - 1 >= 0:\n",
    "                word_lists.append([word] + [text[(i - w - 1)]])\n",
    "            # print(i,word)\n",
    "word_lists\n",
    "\n",
    "baskets\n",
    "\n",
    "\n",
    "def create_unique_word_dict(text: list) -> dict:\n",
    "    \"\"\"\n",
    "\n",
    "    :param text:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    words = list(set(text))\n",
    "\n",
    "    unique_word_dict = {}\n",
    "    for i, word in enumerate(words):\n",
    "        unique_word_dict.update({word: i})\n",
    "\n",
    "    return unique_word_dict\n",
    "\n",
    "\n",
    "unique_word_dict = create_unique_word_dict(all_text)\n",
    "\n",
    "# def create_one_hot_vectors(unique_words:dict)\n",
    "# One-hot-encoding\n",
    "n_words = len(unique_word_dict)\n",
    "words = list(unique_word_dict.keys())\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for word_pair in word_lists:\n",
    "    X.append((word_pair[0] == np.array(words)) * 1)\n",
    "    Y.append((word_pair[1] == np.array(words)) * 1)\n",
    "X = np.asarray(X)\n",
    "Y = np.asarray(X)\n",
    "word_lists\n",
    "X.shape\n",
    "Y.shape\n",
    "from tensorflow import keras\n",
    "from keras import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "\n",
    "# import tensorflow as tf\n",
    "\n",
    "embed_size = 2\n",
    "X.shape[1]\n",
    "inp = Input(shape=X.shape[1], )\n",
    "inp_layer = Dense(embed_size, activation='linear', )(inp)\n",
    "hidden_layer = Dense(units=Y.shape[1], activation='softmax')(inp_layer)\n",
    "\n",
    "model = Model(inputs=inp, outputs=hidden_layer)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.fit(x=X, y=Y, batch_size=256, epochs=1000)\n",
    "\n",
    "weights = model.get_weights()[0]\n",
    "embedding_dict = {}\n",
    "for word in words:\n",
    "    embedding_dict.update({word: weights[unique_word_dict[word]]})\n",
    "embedding_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}