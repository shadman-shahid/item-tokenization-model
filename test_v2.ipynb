{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_json(data,name):\n",
    "    with open(name+'.json', 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "def save_jsonl(data,name):\n",
    "    with open(name+'.jsonl', 'w') as outfile:\n",
    "        for entry in data:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "def load_json(path):\n",
    "    f = open (path, \"r\")\n",
    "    data = json.loads(f.read())\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "def load_jsonl(path):\n",
    "    f = open(path,\"r\").readlines()\n",
    "    data=[]\n",
    "    for i in f:\n",
    "        d = json.loads(i)\n",
    "        data.append(d)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the CPU\n"
     ]
    }
   ],
   "source": [
    "# from ruamel_yaml import YAML\n",
    "# import pytorch_lightning as pl\n",
    "# from omegaconf import DictConfig\n",
    "import IPython\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    device=torch.device(\"cuda:0\")\n",
    "    print(\"Running on the GPU\")\n",
    "    torch.cuda.empty_cache()\n",
    "  #dataType=torch.float32\n",
    "else:\n",
    "    device=torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "  #dataType=torch.float32"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "if sys.platform == 'linux': path_sep = '/'\n",
    "elif sys.platform[:3] == 'win': path_sep = '\\\\'\n",
    "else: path_sep = '/'\n",
    "\n",
    "DATA_PATH = os.path.join(os.getcwd(), 'data')\n",
    "files_list = glob.glob(f'{DATA_PATH}/*.csv')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def get_columns(file_path: str):\n",
    "    '''\n",
    "    Function to read the first line of a csv file and return a list of column names.\n",
    "    :param file_path: file path to csv file\n",
    "    :return: column names\n",
    "    '''\n",
    "    with open(file_path, 'r') as file:\n",
    "        contents = file.readline()\n",
    "    return contents.strip().split(sep=',')\n",
    "\n",
    "def get_product_rows(product_file_path):\n",
    "    with open(product_file_path, 'r') as file:\n",
    "        contents = file.readlines()\n",
    "    columns = get_columns(product_file_path)\n",
    "    for i, line in enumerate(contents[1:]):\n",
    "        words = line.strip().split(sep=', ')\n",
    "        if len(words) > len(columns):\n",
    "           yield {columns[i]: words[i] if i == 0 else ' '.join(words[1:-len(columns)+2]) if i ==1 else words[-(len(columns) - i)] for i in range(len(columns))}\n",
    "        else:\n",
    "            yield {column: word for column, word in zip(columns, words)}\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "products = pd.DataFrame(get_product_rows(\"data/shwapno_dataset_1/products_all.csv\"),)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "ProductCode                                       \"3100877\"\n ProductName      \"X-Deshi Murgi Pack with Gila Koliza (B)\"\n SubCategoryID                                       \"3103\"\n SubCategory                                           Meat\n CategoryID                                            \"31\"\n Category                                           Protein\n UnitPrice                                           0.0000\n VATID                                                   A1\n VAT                                                 0.0000\nName: 1327, dtype: object"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# products = pd.DataFrame(get_product_rows(\"data/shwapno_dataset_1/products_all.csv\"),)\n",
    "columns = get_columns(\"data/shwapno_dataset_1/products_all.csv\")\n",
    "file_path = \"data/shwapno_dataset_1/products_all.csv\"\n",
    "with open(file_path, 'r') as file:\n",
    "    contents = file.readlines()\n",
    "words = contents[1328].strip().split(sep=', ')\n",
    "\n",
    "products.iloc[1327]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "{'ProductCode': '\"3100877\"',\n ' ProductName': 'Koliza (B)\"',\n ' VATID': 'A1',\n ' UnitPrice': '0.0000',\n ' Category': 'Protein',\n ' CategoryID': '\"31\"',\n ' SubCategory': 'Meat',\n ' SubCategoryID': '\"3103\"'}"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "['\"3100877\"',\n '\"X-Deshi Murgi Pack with Gila',\n 'Koliza (B)\"',\n '\"3103\"',\n 'Meat',\n '\"31\"',\n 'Protein',\n '0.0000',\n 'A1',\n '0.0000']"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# products.iloc[1327]\n",
    "words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 9 fields in line 1328, saw 10\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mParserError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# invoice_details = pd.read_csv(\"data/shwapno_dataset_1/invoice_202208_202307.csv\")\u001B[39;00m\n\u001B[1;32m      2\u001B[0m customers \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata/shwapno_dataset_1/customers_202208_202307.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m products \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata/shwapno_dataset_1/products_all.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# invoices_df.head()\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# products.columns = products.iloc[0].to_list()\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# def product_names2codes(names: list):\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m#     return list(map(product_name2code, names))\u001B[39;00m\n",
      "File \u001B[0;32m~/Projects/item-tokenization-model/venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:912\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m    899\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m    900\u001B[0m     dialect,\n\u001B[1;32m    901\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    908\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m    909\u001B[0m )\n\u001B[1;32m    910\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m--> 912\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/item-tokenization-model/venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:583\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    580\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n\u001B[1;32m    582\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m parser:\n\u001B[0;32m--> 583\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mparser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/item-tokenization-model/venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1704\u001B[0m, in \u001B[0;36mTextFileReader.read\u001B[0;34m(self, nrows)\u001B[0m\n\u001B[1;32m   1697\u001B[0m nrows \u001B[38;5;241m=\u001B[39m validate_integer(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnrows\u001B[39m\u001B[38;5;124m\"\u001B[39m, nrows)\n\u001B[1;32m   1698\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1699\u001B[0m     \u001B[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001B[39;00m\n\u001B[1;32m   1700\u001B[0m     (\n\u001B[1;32m   1701\u001B[0m         index,\n\u001B[1;32m   1702\u001B[0m         columns,\n\u001B[1;32m   1703\u001B[0m         col_dict,\n\u001B[0;32m-> 1704\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[attr-defined]\u001B[39;49;00m\n\u001B[1;32m   1705\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnrows\u001B[49m\n\u001B[1;32m   1706\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1707\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m   1708\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/Projects/item-tokenization-model/venv/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001B[0m, in \u001B[0;36mCParserWrapper.read\u001B[0;34m(self, nrows)\u001B[0m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlow_memory:\n\u001B[0;32m--> 234\u001B[0m         chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_low_memory\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    235\u001B[0m         \u001B[38;5;66;03m# destructive to chunks\u001B[39;00m\n\u001B[1;32m    236\u001B[0m         data \u001B[38;5;241m=\u001B[39m _concatenate_chunks(chunks)\n",
      "File \u001B[0;32m~/Projects/item-tokenization-model/venv/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:814\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/Projects/item-tokenization-model/venv/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:875\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._read_rows\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/Projects/item-tokenization-model/venv/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:850\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/Projects/item-tokenization-model/venv/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:861\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/Projects/item-tokenization-model/venv/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:2029\u001B[0m, in \u001B[0;36mpandas._libs.parsers.raise_parser_error\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mParserError\u001B[0m: Error tokenizing data. C error: Expected 9 fields in line 1328, saw 10\n"
     ]
    }
   ],
   "source": [
    "# invoice_details = pd.read_csv(\"data/shwapno_dataset_1/invoice_202208_202307.csv\")\n",
    "customers = pd.read_csv(\"data/shwapno_dataset_1/customers_202208_202307.csv\")\n",
    "products = pd.read_csv(\"data/shwapno_dataset_1/products_all.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# invoices_df.head()\n",
    "#\n",
    "# products.columns = products.iloc[0].to_list()\n",
    "# products = products.drop([0])\n",
    "# products.ProductCode = products.reset_index(drop=True).ProductCode.astype(str)\n",
    "#\n",
    "# def product_code2name(code):\n",
    "#     return products[products.ProductCode==code]['ProductName'].tolist()[-1]\n",
    "#\n",
    "# def product_name2code(name: str):\n",
    "#     return products[products.ProductName==name]['ProductCode'].tolist()[-1]\n",
    "#\n",
    "# def product_codes2names(codes: list):\n",
    "#     print(codes)\n",
    "#     return list(map(product_code2name, codes))\n",
    "#\n",
    "# def product_names2codes(names: list):\n",
    "#     return list(map(product_name2code, names))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "['ProductCode',\n ' ProductName',\n ' SubCategoryID',\n ' SubCategory',\n ' CategoryID',\n ' Category',\n ' UnitPrice',\n ' VATID',\n ' VAT']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_columns(\"data/shwapno_dataset_1/products_all.csv\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [],
   "source": [
    "def enquote_second_column(contents):\n",
    "    # new_lines=[]\n",
    "    for i, line in enumerate(contents):\n",
    "        words = line.split(sep=', ')\n",
    "        if len(words)==5:\n",
    "            # x = line.split(sep='\\\", ')\n",
    "            words[1] = '\"' + words[1] +'\"'\n",
    "            # new_line = {'CustomerCode': words[0],\n",
    "            #             'CustomerName': words[1],\n",
    "            #             'DepotCode': words[2],\n",
    "            #             'AvgNSI': words[], InvCnt}\n",
    "            new_line = ','.join(words)\n",
    "            yield new_line\n",
    "        else:\n",
    "            yield ','.join(words)\n",
    "            # new_lines.append(new_line)\n",
    "            # print(f\"{i+1}th line: {line}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [],
   "source": [
    "def get_rows(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        contents = file.readlines()\n",
    "    for i, line in enumerate(contents):\n",
    "        words = line.split(sep=', ')\n",
    "        if len(words)>5:\n",
    "            yield {'CustomerCode': words[0], 'CustomerName': ' '.join(words[1:-3]), 'DepotCode': words[-3], 'AvgNSI': words[-2], 'InvCnt': words[-1]}\n",
    "        else:\n",
    "            yield {'CustomerCode': words[0], 'CustomerName': words[1], 'DepotCode': words[2], 'AvgNSI': words[3], 'InvCnt': words[4]}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [],
   "source": [
    "## Read from customer csv file. and then convert the last column to integer, AvgNSI column to float, CustomerName customer code and DEPOTCOde to string\n",
    "\n",
    "customers_raw = pd.DataFrame(get_rows(\"data/shwapno_dataset_1/customers_202208_202307.csv\"),)\n",
    "\n",
    "customers = pd.DataFrame(get_rows(\"data/shwapno_dataset_1/customers_202208_202307.csv\"),)\n",
    "customers.InvCnt = customers.InvCnt.apply(lambda x: int(x))\n",
    "customers.AvgNSI = customers.AvgNSI.apply(lambda x: float(x))\n",
    "customers.CustomerName = customers.CustomerName.apply(lambda x: x.strip('\"'))\n",
    "customers.CustomerCode = customers.CustomerCode.apply(lambda x: x.strip('\"'))\n",
    "customers.DepotCode = customers.DepotCode.apply(lambda x: x.strip('\"'))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "data": {
      "text/plain": "['CustomerCode', 'CustomerName', 'DepotCode', 'AvgNSI', 'InvCnt']"
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_columns(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        contents = file.readline()\n",
    "    return contents.strip().split(sep=',')\n",
    "get_columns('./data/shwapno_dataset_1/customers_updated_202208_202307.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50861 entries, 0 to 50860\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   CustomerCode  50861 non-null  int64  \n",
      " 1   CustomerName  50861 non-null  object \n",
      " 2   DepotCode     50861 non-null  object \n",
      " 3   AvgNSI        50861 non-null  float64\n",
      " 4   InvCnt        50861 non-null  int64  \n",
      "dtypes: float64(1), int64(2), object(2)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "## Check whether all entries in the last column in customer dataframe are integers\n",
    "#Check the size of the dataframe curently held in memory\n",
    "# customers.info()\n",
    "# customers.to_csv(\"data/shwapno_dataset_1/customers_updated_202208_202307.csv\", index=False)\n",
    "# customers_updated = pd.read_csv(\"data/shwapno_dataset_1/customers_updated_202208_202307.csv\",)\n",
    "\n",
    "# customers.to_csv(\"data/shwapno_dataset_1/customers_updated_202208_202307.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1313th line: \"Hasan Kabir Sipon(Journalist CANVAS )\",\n",
      "\"01716382856\", \"Hasan Kabir Sipon(Journalist, CANVAS )\", D055, 1319.563518, 3\n",
      "\n",
      "1949th line: \"shofipur kaliakoir,\",\n",
      "\"01773611172\", \"shofipur, kaliakoir,\", F090, 736.112051, 2\n",
      "\n",
      "2320th line: \"MD NANNA\",\n",
      "\"01723718102\", \"MD, NANNA\", F022, 551.793750, 1\n",
      "\n",
      "2443th line: \"Md Omar Farukh\",\n",
      "\"01675545660\", \"Md, Omar Farukh\", D063, 560.501521, 2\n",
      "\n",
      "7371th line: \"Dr. Md. Jahidur Rahman(Professor Department of Pa\",\n",
      "\"01712525873\", \"Dr. Md. Jahidur Rahman(Professor, Department of Pa\", F097, 1186.443602, 5\n",
      "\n",
      "7673th line: \"160/152 college\",\n",
      "\"01511413734\", \"160/152, college\", F113, 1331.940000, 1\n",
      "\n",
      "8080th line: \"164 lalbag\",\n",
      "\"01937596065\", \"164, lalbag\", D045, 802.261688, 2\n",
      "\n",
      "9140th line: \"Mrs Amina Nasrin\",\n",
      "\"13130394\", \"Mrs, Amina Nasrin\", D016, 1109.060136, 3\n",
      "\n",
      "9277th line: \"Mahmuda \",\n",
      "\"01729693485\", \"Mahmuda, \", F002, 787.442459, 2\n",
      "\n",
      "10653th line: \"Nazrul EPZ\",\n",
      "\"01725927032\", \"Nazrul, EPZ\", C011, 1556.667133, 5\n",
      "\n",
      "12334th line: \"House:04 Road:07,\",\n",
      "\"01747404188\", \"House:04, Road:07,\", F050, 688.549436, 7\n",
      "\n",
      "13011th line: \"Rahat 203,East\",\n",
      "\"01670661226\", \"Rahat, 203,East\", D041, 1201.965000, 3\n",
      "\n",
      "17164th line: \"rafiqul house\",\n",
      "\"01921481910\", \"rafiqul, house\", F068, 446.014953, 4\n",
      "\n",
      "17278th line: \"Mr. Nahid H Shiddique\",\n",
      "\"13017845\", \"Mr. Nahid H, Shiddique\", D075, 2119.105766, 5\n",
      "\n",
      "17618th line: \"Shaon \",\n",
      "\"01716626610\", \"Shaon, \", F002, 584.925963, 4\n",
      "\n",
      "17934th line: \"HK-314/4 WEST\",\n",
      "\"01707015000\", \"HK-314/4, WEST\", F053, 1029.397958, 7\n",
      "\n",
      "18514th line: \"Mr.Abu Naser-DO ACILL\",\n",
      "\"14003280\", \"Mr.Abu Naser-DO, ACILL\", D058, 2702.313490, 9\n",
      "\n",
      "18692th line: \"Mashrura \",\n",
      "\"01736196819\", \"Mashrura, \", F002, 1039.017631, 4\n",
      "\n",
      "19067th line: \"h-7 r-116,\",\n",
      "\"13237462\", \"h-7, r-116,\", D076, 1696.816237, 18\n",
      "\n",
      "19282th line: \"kefayet dit\",\n",
      "\"01913003034\", \"kefayet, dit\", F065, 636.539639, 4\n",
      "\n",
      "22456th line: \"shanta dit\",\n",
      "\"01823050459\", \"shanta, dit\", D081, 652.131047, 3\n",
      "\n",
      "23274th line: \"Dr. G. K M Afjal Khan(Senior Consultant Eye)\",\n",
      "\"01712084763\", \"Dr. G. K M Afjal Khan(Senior Consultant, Eye)\", F097, 1434.468315, 3\n",
      "\n",
      "24061th line: \"Rezwan House\",\n",
      "\"01672197464\", \"Rezwan, House\", D054, 591.856451, 3\n",
      "\n",
      "25864th line: \"MR Sanjib\",\n",
      "\"01755581181\", \"MR, Sanjib\", C001, 1044.197741, 4\n",
      "\n",
      "28102th line: \"yesmin 230/2\",\n",
      "\"01839300071\", \"yesmin, 230/2\", D004, 582.723000, 2\n",
      "\n",
      "28753th line: \"rehan ka\",\n",
      "\"18100698\", \"rehan, ka\", D068, 578.632931, 2\n",
      "\n",
      "30235th line: \"mrs nila\",\n",
      "\"13260134\", \"mrs, nila\", D063, 573.411290, 5\n",
      "\n",
      "31984th line: \"Mr. Md. Morshed Alam\",\n",
      "\"19029333\", \"Mr. Md., Morshed Alam\", D031, 1440.785959, 9\n",
      "\n",
      "32430th line: \"mahfuj 229\",\n",
      "\"01973610043\", \"mahfuj, 229\", D045, 333.057241, 6\n",
      "\n",
      "33364th line: \"BabulDIT Project Merul Badda\",\n",
      "\"01720140890\", \"BabulDIT Project, Merul Badda\", D081, 953.279736, 2\n",
      "\n",
      "33495th line: \"Lakkhipura Joydebpur,\",\n",
      "\"01674618417\", \"Lakkhipura, Joydebpur,\", F053, 1526.749841, 2\n",
      "\n",
      "33566th line: \"Mezba New\",\n",
      "\"01816347232\", \"Mezba, New\", C001, 1005.608208, 3\n",
      "\n",
      "34821th line: \"Mr. Md Emtiaz Mahamud\",\n",
      "\"13224292\", \"Mr. Md, Emtiaz Mahamud\", D071, 1426.974193, 2\n",
      "\n",
      "37399th line: \"MD.Moniruzzaman(Deputy General Manager  Orbitec B\",\n",
      "\"01778045540\", \"MD.Moniruzzaman(Deputy General Manager , Orbitec B\", D075, 1651.817721, 31\n",
      "\n",
      "37539th line: \"Md.Halimuzzaman gulshan\",\n",
      "\"01911218845\", \"Md.Halimuzzaman, gulshan\", F060, 1093.307636, 4\n",
      "\n",
      "37890th line: \"Mr. Siddiqur Rahman Sr.Manager (The Daily Star)\",\n",
      "\"01711623913\", \"Mr. Siddiqur Rahman, Sr.Manager (The Daily Star)\", D075, 1720.209400, 11\n",
      "\n",
      "37989th line: \"Shamima akter,\",\n",
      "\"01711031304\", \"Shamima, akter,\", D031, 1405.309711, 4\n",
      "\n",
      "38925th line: \"West Shewrapara Wasa Road\",\n",
      "\"01763388039\", \"West Shewrapara, Wasa Road\", F124, 407.778421, 1\n",
      "\n",
      "39698th line: \"sayad krishi\",\n",
      "\"01716104149\", \"sayad, krishi\", F002, 865.159375, 1\n",
      "\n",
      "42583th line: \"Md Shahadat Hossain Parosh\",\n",
      "\"01818225756\", \"Md, Shahadat Hossain Parosh\", D062, 2099.431868, 3\n",
      "\n",
      "42963th line: \"Md Sharitul Islam\",\n",
      "\"19026948\", \"Md, Sharitul Islam\", F164, 932.033377, 5\n",
      "\n",
      "43325th line: \"Prof. Dr. Moonmoon Rahman(MBBS DBS&T (DU))\",\n",
      "\"21003983\", \"Prof. Dr. Moonmoon Rahman(MBBS, DBS&T (DU))\", D031, 2199.242000, 4\n",
      "\n",
      "43397th line: \"Dr. Akter Banu Beauty(Assistant Professor Departm\",\n",
      "\"01717413369\", \"Dr. Akter Banu Beauty(Assistant Professor, Departm\", F097, 1409.137218, 4\n",
      "\n",
      "43922th line: \"MR KAJOL\",\n",
      "\"01792079658\", \"MR, KAJOL\", D014, 305.967735, 4\n",
      "\n",
      "45690th line: \"Md. MubarakTolarbag Mirpur\",\n",
      "\"01675422911\", \"Md. MubarakTolarbag, Mirpur\", D070, 1526.308437, 4\n",
      "\n",
      "46210th line: \"Khabir UddinDIT Project Merul Badda\",\n",
      "\"01755644524\", \"Khabir UddinDIT Project, Merul Badda\", D081, 1156.790714, 2\n",
      "\n",
      "49448th line: \"Dr. Sharmila Sarker Ruma(Medical Officer Banglade\",\n",
      "\"01742117797\", \"Dr. Sharmila Sarker Ruma(Medical Officer, Banglade\", F097, 1240.299838, 4\n",
      "\n",
      "50083th line: \"md.emdadullah hasan\",\n",
      "\"01913736276\", \"md.emdadullah, hasan\", F101, 1103.941000, 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/shwapno_dataset_1/customers_202208_202307.csv\", 'r') as file:\n",
    "    contents = file.readlines()\n",
    "\n",
    "for i, line in enumerate(contents):\n",
    "    words = line.split(sep=', ')\n",
    "    if len(words)>5:\n",
    "        # x = line.split(sep='\\\", ')\n",
    "       print(f\"{i+1}th line: {customers.iloc[i][1]},\\n{line}\")\n",
    "\n",
    "# x = contents[1312].split(sep=', ')\n",
    "# ' '.join(x[1:-3]), x[-3]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [],
   "source": [
    "with open(\"data/shwapno_dataset_1/temp2.csv\", \"w\") as file:\n",
    "    file.writelines(enquote_second_column(contents))\n",
    "\n",
    "# new_contents.split(sep=\"\\n\")[1309*2:1314*2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [],
   "source": [
    "with open(\"data/shwapno_dataset_1/temp2.csv\", \"r\") as file:\n",
    "    contents_2 = file.readlines()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "data": {
      "text/plain": "'\"01938800900\",\"\"mr.jasim.khan h-30,r-02,b-b\"\",D049,470.860660,3\\n'"
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents_2[561]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 5 fields in line 562, saw 7\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mParserError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[151], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m customers \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata/shwapno_dataset_1/temp2.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/item-tokenization-model/venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:912\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m    899\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m    900\u001B[0m     dialect,\n\u001B[1;32m    901\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    908\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m    909\u001B[0m )\n\u001B[1;32m    910\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m--> 912\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/item-tokenization-model/venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:583\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    580\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n\u001B[1;32m    582\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m parser:\n\u001B[0;32m--> 583\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mparser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/item-tokenization-model/venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1704\u001B[0m, in \u001B[0;36mTextFileReader.read\u001B[0;34m(self, nrows)\u001B[0m\n\u001B[1;32m   1697\u001B[0m nrows \u001B[38;5;241m=\u001B[39m validate_integer(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnrows\u001B[39m\u001B[38;5;124m\"\u001B[39m, nrows)\n\u001B[1;32m   1698\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1699\u001B[0m     \u001B[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001B[39;00m\n\u001B[1;32m   1700\u001B[0m     (\n\u001B[1;32m   1701\u001B[0m         index,\n\u001B[1;32m   1702\u001B[0m         columns,\n\u001B[1;32m   1703\u001B[0m         col_dict,\n\u001B[0;32m-> 1704\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[attr-defined]\u001B[39;49;00m\n\u001B[1;32m   1705\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnrows\u001B[49m\n\u001B[1;32m   1706\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1707\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m   1708\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/Projects/item-tokenization-model/venv/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001B[0m, in \u001B[0;36mCParserWrapper.read\u001B[0;34m(self, nrows)\u001B[0m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlow_memory:\n\u001B[0;32m--> 234\u001B[0m         chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_low_memory\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    235\u001B[0m         \u001B[38;5;66;03m# destructive to chunks\u001B[39;00m\n\u001B[1;32m    236\u001B[0m         data \u001B[38;5;241m=\u001B[39m _concatenate_chunks(chunks)\n",
      "File \u001B[0;32m~/Projects/item-tokenization-model/venv/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:814\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/Projects/item-tokenization-model/venv/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:875\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._read_rows\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/Projects/item-tokenization-model/venv/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:850\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/Projects/item-tokenization-model/venv/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:861\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/Projects/item-tokenization-model/venv/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:2029\u001B[0m, in \u001B[0;36mpandas._libs.parsers.raise_parser_error\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mParserError\u001B[0m: Error tokenizing data. C error: Expected 5 fields in line 562, saw 7\n"
     ]
    }
   ],
   "source": [
    "customers = pd.read_csv(\"data/shwapno_dataset_1/temp2.csv\",)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\", \n"
     ]
    }
   ],
   "source": [
    "print('\\\", ')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "array([2300011, 2300139, 2300438, ..., 3808633, 2303838, 2800614])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoice_details.ProductCode.unique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "        ProductCode                            ProductName SubCategoryID  \\\n1           4214666               Sugar Loose Refined (Kg)          2402   \n2           2300035            S201372 SUPTA 3 COLOR SLIME          4203   \n3           2603741                 7 Up Pet Bottle 2000ml          2301   \n4           2300782  Mr.White Detergent Powder 500g(B3G1F)          2601   \n5           2400643                Coca Cola 2.25 Ltr. pet          2301   \n...             ...                                    ...           ...   \n3833730         NaN                S200798 Baby Woolen Cap          3301   \n3833788         NaN  Eon Food Ocean Ria Fish Sandwich 500g          2809   \n3837228         NaN            S201047 R_OVEN DISH- 8078-3          3801   \n3840896         NaN         S202296  Combo Hijab 3 pcs Set          3301   \n3848562         NaN           S200476 Kiam Disco Hari No-9          3801   \n\n              SubCategory CategoryID            Category UnitPrice VATID  \\\n1              Essentials         24         Commodities  136.0000    A1   \n2                    Toys         42         Gift & Toys   99.0000    A6   \n3        Carbonated Drink         23  Beverage & Tobacco  130.0000    A6   \n4             Fabric Care         26           Home Care  285.0000    A6   \n5        Carbonated Drink         23  Beverage & Tobacco  140.0000    A6   \n...                   ...        ...                 ...       ...   ...   \n3833730        Clothes-33         33          Life Style     400.0    A7   \n3833788            Snacks         28      Packaged Foods     690.0    A6   \n3837228       Accessories         38    Home Accessories     775.0    A6   \n3840896        Clothes-33         33          Life Style     449.0    A7   \n3848562       Accessories         38    Home Accessories     369.0    A6   \n\n             VAT  \n1         0.0000  \n2         4.9500  \n3         6.5000  \n4        14.2500  \n5         7.0000  \n...          ...  \n3833730     30.0  \n3833788     34.5  \n3837228    38.75  \n3840896   33.675  \n3848562    18.45  \n\n[22926 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ProductCode</th>\n      <th>ProductName</th>\n      <th>SubCategoryID</th>\n      <th>SubCategory</th>\n      <th>CategoryID</th>\n      <th>Category</th>\n      <th>UnitPrice</th>\n      <th>VATID</th>\n      <th>VAT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>4214666</td>\n      <td>Sugar Loose Refined (Kg)</td>\n      <td>2402</td>\n      <td>Essentials</td>\n      <td>24</td>\n      <td>Commodities</td>\n      <td>136.0000</td>\n      <td>A1</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2300035</td>\n      <td>S201372 SUPTA 3 COLOR SLIME</td>\n      <td>4203</td>\n      <td>Toys</td>\n      <td>42</td>\n      <td>Gift &amp; Toys</td>\n      <td>99.0000</td>\n      <td>A6</td>\n      <td>4.9500</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2603741</td>\n      <td>7 Up Pet Bottle 2000ml</td>\n      <td>2301</td>\n      <td>Carbonated Drink</td>\n      <td>23</td>\n      <td>Beverage &amp; Tobacco</td>\n      <td>130.0000</td>\n      <td>A6</td>\n      <td>6.5000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2300782</td>\n      <td>Mr.White Detergent Powder 500g(B3G1F)</td>\n      <td>2601</td>\n      <td>Fabric Care</td>\n      <td>26</td>\n      <td>Home Care</td>\n      <td>285.0000</td>\n      <td>A6</td>\n      <td>14.2500</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2400643</td>\n      <td>Coca Cola 2.25 Ltr. pet</td>\n      <td>2301</td>\n      <td>Carbonated Drink</td>\n      <td>23</td>\n      <td>Beverage &amp; Tobacco</td>\n      <td>140.0000</td>\n      <td>A6</td>\n      <td>7.0000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3833730</th>\n      <td>NaN</td>\n      <td>S200798 Baby Woolen Cap</td>\n      <td>3301</td>\n      <td>Clothes-33</td>\n      <td>33</td>\n      <td>Life Style</td>\n      <td>400.0</td>\n      <td>A7</td>\n      <td>30.0</td>\n    </tr>\n    <tr>\n      <th>3833788</th>\n      <td>NaN</td>\n      <td>Eon Food Ocean Ria Fish Sandwich 500g</td>\n      <td>2809</td>\n      <td>Snacks</td>\n      <td>28</td>\n      <td>Packaged Foods</td>\n      <td>690.0</td>\n      <td>A6</td>\n      <td>34.5</td>\n    </tr>\n    <tr>\n      <th>3837228</th>\n      <td>NaN</td>\n      <td>S201047 R_OVEN DISH- 8078-3</td>\n      <td>3801</td>\n      <td>Accessories</td>\n      <td>38</td>\n      <td>Home Accessories</td>\n      <td>775.0</td>\n      <td>A6</td>\n      <td>38.75</td>\n    </tr>\n    <tr>\n      <th>3840896</th>\n      <td>NaN</td>\n      <td>S202296  Combo Hijab 3 pcs Set</td>\n      <td>3301</td>\n      <td>Clothes-33</td>\n      <td>33</td>\n      <td>Life Style</td>\n      <td>449.0</td>\n      <td>A7</td>\n      <td>33.675</td>\n    </tr>\n    <tr>\n      <th>3848562</th>\n      <td>NaN</td>\n      <td>S200476 Kiam Disco Hari No-9</td>\n      <td>3801</td>\n      <td>Accessories</td>\n      <td>38</td>\n      <td>Home Accessories</td>\n      <td>369.0</td>\n      <td>A6</td>\n      <td>18.45</td>\n    </tr>\n  </tbody>\n</table>\n<p>22926 rows Ã— 9 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "return_condition = invoice_details.SalesQTY<=0\n",
    "invoice_details = invoice_details[~return_condition]\n",
    "invoice_details['SalesQTY'] = invoice_details['SalesQTY'].apply(lambda x: max((1, np.floor(x))))\n",
    "\n",
    "invoice_details.ProductCode = invoice_details.ProductCode.astype(str)\n",
    "\n",
    "invoice_details['new_product']\n",
    "\n",
    "baskets = invoice_details.loc[invoice_details.index.repeat(invoice_details['SalesQTY'])].groupby('InvoiceNo')['ProductCode'].apply(list)\n",
    "\n",
    "\n",
    "other_elements = invoice_details[['InvoiceNo','CustomerCode','PrepareDate', 'ProductNSI', 'NSI', 'SalesQTY', 'Cash_1_Card_0', 'InvoicePeriod', 'InvoiceDate','DepotCode']].drop_duplicates().set_index('InvoiceNo')\n",
    "other_elements['PrepareDate'] = pd.to_datetime(other_elements.PrepareDate)\n",
    "invoice_details.groupby(['CustomerCode', 'ProductCode']).InvoiceNo.count(sor    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class ShwapnoDataset(Dataset):\n",
    "    def __init__(self, csv_files, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_files (dict): list of Path to the csv files of invoices, customers and products.\n",
    "            root_dir (string): Directory with all the csv-s.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "\n",
    "        self.invoices = pd.read_csv(csv_files['invoices'])\n",
    "        self.customers = pd.read_csv(csv_files['customers'])\n",
    "        self.products = pd.read_csv(csv_files['products'])\n",
    "\n",
    "        return_condition = self.invoices.SalesQTY<=0\n",
    "        self.invoices = self.invoices[~return_condition]\n",
    "        self.invoices['SalesQTY'] = self.invoices['SalesQTY'].apply(lambda x: max((1, np.floor(x))))\n",
    "\n",
    "        self.invoices.ProductCode = invoice_details.ProductCode.astype(str)\n",
    "        self.baskets = self.invoices.loc[invoice_details.index.repeat(invoice_details['SalesQTY'])].groupby('InvoiceNo')['ProductCode'].apply(list)\n",
    "\n",
    "\n",
    "        other_elements = invoice_details[['InvoiceNo','CustomerCode','PrepareDate', 'ProductNSI', 'NSI', 'SalesQTY', 'Cash_1_Card_0', 'InvoicePeriod', 'InvoiceDate','DepotCode']].drop_duplicates().set_index('InvoiceNo')\n",
    "        other_elements['PrepareDate'] = pd.to_datetime(other_elements.PrepareDate)\n",
    "\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.invoices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.landmarks_frame.iloc[idx, 0])\n",
    "        image = io.imread(img_name)\n",
    "        landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
    "        landmarks = np.array([landmarks], dtype=float).reshape(-1, 2)\n",
    "        sample = {'image': image, 'landmarks': landmarks}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def product_code2name(code):\n",
    "        return products[products.ProductCode==code]['ProductName'].tolist()[-1]\n",
    "\n",
    "    def product_name2code(name: str):\n",
    "        return products[products.ProductName==name]['ProductCode'].tolist()[-1]\n",
    "\n",
    "    def product_codes2names(codes: list):\n",
    "        print(codes)\n",
    "        return list(map(product_code2name, codes))\n",
    "\n",
    "    def product_names2codes(names: list):\n",
    "        return list(map(product_name2code, names))\n",
    "\n",
    "    def create_basket(self):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=False):\n",
    "        \"\"\"\n",
    "        Construct a Dataset object to iterate over data X and labels y\n",
    "\n",
    "        Inputs:\n",
    "        - X: Numpy array of data, of any shape\n",
    "        - y: Numpy array of labels, of any shape but with y.shape[0] == X.shape[0]\n",
    "        - batch_size: Integer giving number of elements per minibatch\n",
    "        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch\n",
    "        \"\"\"\n",
    "        assert X.shape[0] == y.shape[0], 'Got different numbers of data and labels'\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        N, B = self.X.shape[0], self.batch_size\n",
    "        idxs = np.arange(N)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
